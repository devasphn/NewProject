# RunPod Configuration for Telugu S2S Training & Deployment
# Complete setup for H200 training and A6000 inference

# ==============================================================================
# TRAINING CONFIGURATION (H200 SXM)
# ==============================================================================

training_pod:
  # GPU Selection
  gpu_type: "H200 SXM"
  gpu_count: 1
  gpu_specs:
    memory: "141GB HBM3e"
    bandwidth: "4.8 TB/s"
    compute: "FP16: 989 TFLOPS"
    price: "$3.89/hour"  # RunPod pricing
  
  # Pod Configuration
  template: "RunPod PyTorch 2.2"
  container_image: "runpod/pytorch:2.2.0-py3.10-cuda12.1.0-devel-ubuntu22.04"
  
  # Storage
  volume_size: "200GB"  # For dataset and checkpoints
  persistent_storage: true
  
  # System Requirements
  system:
    cpu_cores: 24
    ram: "200GB"
    network: "10 Gbps"
  
  # Environment Setup Script
  setup_commands: |
    # Update system
    apt-get update && apt-get install -y \
      ffmpeg \
      git \
      vim \
      tmux \
      htop \
      nvtop \
      wget \
      curl
    
    # Install Python packages
    pip install --upgrade pip
    pip install \
      torch==2.2.0 \
      torchaudio \
      transformers==4.45.0 \
      accelerate==0.33.0 \
      flash-attn==2.5.0 \
      deepspeed \
      wandb \
      einops \
      datasets \
      soundfile \
      librosa \
      webrtcvad \
      pyyaml \
      tensorboard
    
    # Install WhisperX for alignment
    pip install git+https://github.com/m-bain/whisperX.git
    
    # Install yt-dlp for data collection
    pip install yt-dlp
    
    # Setup Weights & Biases
    wandb login --relogin $WANDB_API_KEY
  
  # Environment Variables
  environment_variables:
    CUDA_VISIBLE_DEVICES: "0"
    HF_TOKEN: "${HF_TOKEN}"
    WANDB_API_KEY: "${WANDB_API_KEY}"
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
    CUDA_LAUNCH_BLOCKING: "0"
    TORCH_CUDA_ARCH_LIST: "9.0"  # H200 architecture

# ==============================================================================
# CODEC TRAINING JOB
# ==============================================================================

codec_training:
  script: "train_codec.py"
  
  # Hyperparameters optimized for H200
  hyperparameters:
    batch_size: 32  # H200 can handle large batches
    gradient_accumulation: 4
    effective_batch_size: 128
    learning_rate: 1e-4
    warmup_steps: 1000
    num_epochs: 100
    
  # Data Configuration
  data:
    train_hours: 80
    val_hours: 10
    test_hours: 10
    segment_duration: 2.0  # seconds
    sample_rate: 16000
  
  # Optimization Settings
  optimization:
    mixed_precision: "bf16"  # H200 native BF16
    gradient_checkpointing: true
    compile_model: true  # torch.compile()
    flash_attention: true
    deepspeed_stage: 2  # ZeRO stage 2
  
  # Expected Performance
  expected:
    duration: "6-8 hours"
    cost: "$24-32"
    final_loss: "<0.01"
    snr: ">30 dB"
    bitrate: "16 kbps"

# ==============================================================================
# S2S MODEL TRAINING JOB
# ==============================================================================

s2s_training:
  script: "train_s2s.py"
  
  # Model Configuration
  model:
    type: "TeluguS2STransformer"
    parameters: "300M"
    hidden_dim: 768
    num_heads: 12
    num_layers: 12
    vocab_size: 1024
    num_quantizers: 8
  
  # Training Configuration
  hyperparameters:
    batch_size: 8
    gradient_accumulation: 8
    effective_batch_size: 64
    learning_rate: 5e-5
    warmup_steps: 5000
    total_steps: 150000
    max_seq_length: 4096
  
  # Data Configuration  
  data:
    conversational_pairs: 50000
    total_hours: 100
    speakers: 4
    emotions: 9
    augmentation:
      speed_perturbation: [0.9, 1.0, 1.1]
      noise_addition: true
      reverb: true
  
  # Optimization
  optimization:
    mixed_precision: "bf16"
    gradient_checkpointing: true
    flash_attention: true
    kv_cache_optimization: true
    torch_compile: true
    deepspeed:
      stage: 2
      offload_optimizer: false
      offload_param: false
  
  # Expected Performance
  expected:
    duration: "18-24 hours"
    cost: "$70-95"
    perplexity: "<10"
    latency: "<150ms"
    quality_mos: ">4.0"

# ==============================================================================
# DEPLOYMENT CONFIGURATION (RTX A6000)
# ==============================================================================

inference_pod:
  # GPU Selection
  gpu_type: "RTX A6000"
  gpu_count: 1
  gpu_specs:
    memory: "48GB GDDR6"
    bandwidth: "768 GB/s"
    compute: "FP16: 38.7 TFLOPS"
    price: "$0.49/hour"
  
  # Pod Configuration
  template: "RunPod Fast Stable Diffusion"  # Good for inference
  container_image: "runpod/pytorch:2.2.0-py3.10-cuda11.8.0-runtime-ubuntu22.04"
  
  # Storage
  volume_size: "50GB"  # Just for models
  persistent_storage: true
  
  # Network Configuration
  exposed_ports:
    - 8000  # FastAPI server
    - 8001  # WebSocket
    - 6006  # TensorBoard
  
  # Auto-scaling
  autoscale:
    enabled: true
    min_workers: 1
    max_workers: 5
    target_gpu_utilization: 70
  
  # Deployment Script
  startup_script: |
    #!/bin/bash
    cd /workspace
    
    # Clone repository
    git clone https://github.com/devasphn/telugu-s2s.git
    cd telugu-s2s
    
    # Download models from HuggingFace
    python download_models.py
    
    # Start server with optimizations
    export CUDA_VISIBLE_DEVICES=0
    export OMP_NUM_THREADS=8
    export MKL_NUM_THREADS=8
    
    # Launch with optimal settings
    python server.py \
      --host 0.0.0.0 \
      --port 8000 \
      --workers 1 \
      --batch_size 1 \
      --use_kv_cache \
      --use_flash_attn \
      --compile_model

# ==============================================================================
# MONITORING & LOGGING
# ==============================================================================

monitoring:
  # Weights & Biases
  wandb:
    project: "telugu-s2s"
    entity: "${WANDB_ENTITY}"
    tags: ["production", "telugu", "s2s"]
  
  # TensorBoard
  tensorboard:
    logdir: "/workspace/logs"
    port: 6006
  
  # Metrics to Track
  metrics:
    training:
      - loss
      - learning_rate
      - gradient_norm
      - throughput_samples_per_second
      - gpu_utilization
      - gpu_memory_usage
    
    inference:
      - latency_ms
      - throughput_requests_per_second
      - active_connections
      - cache_hit_rate
      - memory_usage

# ==============================================================================
# DATA PIPELINE
# ==============================================================================

data_pipeline:
  # Collection Sources
  sources:
    youtube:
      channels:
        - "Raw Talks with VK"
        - "10TV Telugu"
        - "Sakshi TV"
        - "NTV Telugu"
      total_hours: 100
      quality_filter: ">=128kbps"
    
  # Processing Pipeline
  processing:
    - step: "download"
      tool: "yt-dlp"
      format: "wav"
      sample_rate: 16000
    
    - step: "segmentation"
      tool: "WhisperX"
      language: "te"
      min_duration: 2.0
      max_duration: 30.0
    
    - step: "filtering"
      criteria:
        snr: ">20dB"
        confidence: ">0.8"
    
    - step: "augmentation"
      methods:
        - speed_perturbation
        - noise_injection
        - reverb_addition

# ==============================================================================
# OPTIMIZATION PROFILES
# ==============================================================================

optimization_profiles:
  # Maximum Speed (for demo)
  ultra_fast:
    batch_size: 1
    use_kv_cache: true
    use_flash_attn: true
    compile_model: true
    quantization: "int8"
    target_latency: "<100ms"
    quality_tradeoff: "acceptable"
  
  # Balanced (production)
  balanced:
    batch_size: 4
    use_kv_cache: true
    use_flash_attn: true
    compile_model: true
    quantization: "none"
    target_latency: "<150ms"
    quality_tradeoff: "minimal"
  
  # Maximum Quality
  high_quality:
    batch_size: 1
    use_kv_cache: true
    use_flash_attn: false
    compile_model: false
    quantization: "none"
    target_latency: "<300ms"
    quality_tradeoff: "none"

# ==============================================================================
# DEPLOYMENT COMMANDS
# ==============================================================================

deployment_commands:
  # Quick Start (Training)
  start_training: |
    runpodctl create pod \
      --name "telugu-s2s-training" \
      --gpu-type "H200 SXM" \
      --gpu-count 1 \
      --container-image "runpod/pytorch:2.2.0-py3.10-cuda12.1.0-devel-ubuntu22.04" \
      --volume-size 200 \
      --ports "22:22,6006:6006" \
      --env "HF_TOKEN=$HF_TOKEN,WANDB_API_KEY=$WANDB_API_KEY"
  
  # Quick Start (Inference)
  start_inference: |
    runpodctl create pod \
      --name "telugu-s2s-inference" \
      --gpu-type "RTX A6000" \
      --gpu-count 1 \
      --container-image "runpod/pytorch:2.2.0-py3.10-cuda11.8.0-runtime-ubuntu22.04" \
      --volume-size 50 \
      --ports "8000:8000,8001:8001" \
      --min-workers 1 \
      --max-workers 5
  
  # SSH into pod
  ssh_command: |
    ssh root@[POD_ID].runpod.io -p 22 -i ~/.ssh/runpod_key
  
  # Copy files to pod
  copy_to_pod: |
    rsync -avz --progress ./telugu_data root@[POD_ID].runpod.io:/workspace/
  
  # Monitor training
  monitor: |
    tensorboard --logdir=/workspace/logs --host=0.0.0.0 --port=6006

# ==============================================================================
# COST BREAKDOWN
# ==============================================================================

cost_analysis:
  # One-time Training Costs
  training:
    codec_training:
      duration: "8 hours"
      gpu: "H200 SXM"
      cost: "$31.12"
    
    s2s_training:
      duration: "24 hours"
      gpu: "H200 SXM"
      cost: "$93.36"
    
    fine_tuning:
      duration: "4 hours"
      gpu: "H200 SXM"
      cost: "$15.56"
    
    total_training: "$140.04"
  
  # Ongoing Inference Costs
  inference:
    gpu: "RTX A6000"
    price_per_hour: "$0.49"
    users_per_gpu: 100
    cost_per_user_hour: "$0.0049"
    monthly_cost_100_users: "$352.80"  # 24/7 operation
  
  # Total Investment
  total:
    initial: "$140.04"
    monthly: "$352.80"
    cost_per_1000_requests: "$0.12"  # Assuming 30s per request

# ==============================================================================
# SUCCESS CRITERIA
# ==============================================================================

success_criteria:
  # Performance
  latency:
    first_token: "<150ms"
    streaming: "real-time factor <0.8"
  
  # Quality
  quality:
    mos_score: ">4.0/5.0"
    telugu_accuracy: ">90%"
    emotion_recognition: ">85%"
  
  # Scalability
  scalability:
    concurrent_users: ">100 per GPU"
    throughput: ">1000 requests/hour"
  
  # Business
  business:
    better_than_luna_demo: true
    production_ready: true
    cost_effective: true