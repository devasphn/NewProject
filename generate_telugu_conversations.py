#!/usr/bin/env python3
"""
Generate Telugu Conversation Training Data
==========================================

Creates (question, answer) audio pairs for training S2S conversation model.

Methods:
1. Synthetic: Use predefined Q&A pairs + TTS
2. LLM-generated: Use Qwen to generate Q&A, then TTS
3. Real data: Process Kathbath/other datasets

Output format:
data/telugu_conversations/
â”œâ”€â”€ pair_0001/
â”‚   â”œâ”€â”€ question.wav
â”‚   â”œâ”€â”€ answer.wav
â”‚   â”œâ”€â”€ question_codes.pt  # Encoded with YOUR codec
â”‚   â””â”€â”€ answer_codes.pt
â”œâ”€â”€ pair_0002/
â”‚   â””â”€â”€ ...
"""

import torch
import numpy as np
import os
import json
import asyncio
import argparse
from pathlib import Path
from tqdm import tqdm
import random

# Telugu conversation templates
TELUGU_QA_PAIRS = [
    # Greetings
    ("à°¨à°®à°¸à±à°•à°¾à°°à°‚", "à°¨à°®à°¸à±à°•à°¾à°°à°‚! à°®à±€à°•à± à°à°²à°¾ à°¸à°¹à°¾à°¯à°‚ à°šà±‡à°¯à°—à°²à°¨à±?"),
    ("à°¹à°²à±‹", "à°¹à°²à±‹! à°šà±†à°ªà±à°ªà°‚à°¡à°¿, à°®à±€à°•à± à°à°®à°¿ à°•à°¾à°µà°¾à°²à°¿?"),
    ("à°¶à±à°­à±‹à°¦à°¯à°‚", "à°¶à±à°­à±‹à°¦à°¯à°‚! à°®à±€ à°°à±‹à°œà± à°¬à°¾à°—à±à°‚à°¡à°¾à°²à°¨à°¿ à°•à±‹à°°à±à°•à±à°‚à°Ÿà±à°¨à±à°¨à°¾à°¨à±."),
    ("à°¶à±à°­ à°¸à°¾à°¯à°‚à°¤à±à°°à°‚", "à°¶à±à°­ à°¸à°¾à°¯à°‚à°¤à±à°°à°‚! à°à°²à°¾ à°‰à°¨à±à°¨à°¾à°°à±?"),
    
    # How are you
    ("à°®à±€à°°à± à°à°²à°¾ à°‰à°¨à±à°¨à°¾à°°à±?", "à°¨à±‡à°¨à± à°¬à°¾à°—à±à°¨à±à°¨à°¾à°¨à±, à°§à°¨à±à°¯à°µà°¾à°¦à°¾à°²à±! à°®à±€à°°à± à°à°²à°¾ à°‰à°¨à±à°¨à°¾à°°à±?"),
    ("à°à°²à°¾ à°‰à°¨à±à°¨à°¾à°µà±?", "à°¬à°¾à°—à±à°¨à±à°¨à°¾à°¨à±! à°®à±€ à°—à±à°°à°¿à°‚à°šà°¿ à°šà±†à°ªà±à°ªà°‚à°¡à°¿."),
    ("à°•à±à°·à±‡à°®à°‚à°—à°¾ à°‰à°¨à±à°¨à°¾à°°à°¾?", "à°šà°¾à°²à°¾ à°•à±à°·à±‡à°®à°‚à°—à°¾ à°‰à°¨à±à°¨à°¾à°¨à±, à°®à±€à°°à±?"),
    
    # Name queries
    ("à°®à±€ à°ªà±‡à°°à± à°à°®à°¿à°Ÿà°¿?", "à°¨à°¾ à°ªà±‡à°°à± à°¤à±†à°²à±à°—à± à°…à°¸à°¿à°¸à±à°Ÿà±†à°‚à°Ÿà±. à°®à±€à°•à± à°¸à°¹à°¾à°¯à°‚ à°šà±‡à°¯à°¡à°¾à°¨à°¿à°•à°¿ à°‡à°•à±à°•à°¡ à°‰à°¨à±à°¨à°¾à°¨à±."),
    ("à°¨à±€ à°ªà±‡à°°à± à°šà±†à°ªà±à°ªà±", "à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à± à°µà°¾à°¯à°¿à°¸à± à°…à°¸à°¿à°¸à±à°Ÿà±†à°‚à°Ÿà±à°¨à°¿."),
    ("à°¨à±à°µà±à°µà± à°à°µà°°à±?", "à°¨à±‡à°¨à± à°®à±€ à°¤à±†à°²à±à°—à± AI à°…à°¸à°¿à°¸à±à°Ÿà±†à°‚à°Ÿà±à°¨à°¿."),
    
    # Weather
    ("à°µà°¾à°¤à°¾à°µà°°à°£à°‚ à°à°²à°¾ à°‰à°‚à°¦à°¿?", "à°ˆ à°°à±‹à°œà± à°µà°¾à°¤à°¾à°µà°°à°£à°‚ à°¬à°¾à°—à±à°‚à°¦à°¿. à°®à±€ à°ªà±à°°à°¾à°‚à°¤à°‚à°²à±‹ à°à°²à°¾ à°‰à°‚à°¦à°¿?"),
    ("à°ˆ à°°à±‹à°œà± à°µà°°à±à°·à°‚ à°µà°¸à±à°¤à±à°‚à°¦à°¾?", "à°µà°¾à°¤à°¾à°µà°°à°£ à°¸à°®à°¾à°šà°¾à°°à°‚ à°ªà±à°°à°•à°¾à°°à°‚ à°šà±†à°ªà±à°ªà°—à°²à°¨à±."),
    
    # Time
    ("à°¸à°®à°¯à°‚ à°à°‚à°¤?", "à°ªà±à°°à°¸à±à°¤à±à°¤ à°¸à°®à°¯à°‚ à°šà±†à°ªà±à°ªà°—à°²à°¨à±."),
    ("à°ˆ à°°à±‹à°œà± à° à°¤à±‡à°¦à±€?", "à°ˆ à°°à±‹à°œà± à°¤à±‡à°¦à±€ à°šà±†à°ªà±à°ªà°—à°²à°¨à±."),
    
    # Help
    ("à°¨à°¾à°•à± à°¸à°¹à°¾à°¯à°‚ à°•à°¾à°µà°¾à°²à°¿", "à°¤à°ªà±à°ªà°•à±à°‚à°¡à°¾! à°®à±€à°•à± à° à°µà°¿à°§à°‚à°—à°¾ à°¸à°¹à°¾à°¯à°‚ à°šà±‡à°¯à°—à°²à°¨à±?"),
    ("à°¨à±€à°µà± à°à°®à°¿ à°šà±‡à°¯à°—à°²à°µà±?", "à°¨à±‡à°¨à± à°®à±€à°¤à±‹ à°¤à±†à°²à±à°—à±à°²à±‹ à°®à°¾à°Ÿà±à°²à°¾à°¡à°—à°²à°¨à±, à°ªà±à°°à°¶à±à°¨à°²à°•à± à°¸à°®à°¾à°§à°¾à°¨à°¾à°²à± à°‡à°µà±à°µà°—à°²à°¨à±."),
    
    # Thank you
    ("à°§à°¨à±à°¯à°µà°¾à°¦à°¾à°²à±", "à°®à±€à°•à± à°¸à±à°µà°¾à°—à°¤à°‚! à°®à°°à±‡à°¦à±ˆà°¨à°¾ à°¸à°¹à°¾à°¯à°‚ à°•à°¾à°µà°¾à°²à°¾?"),
    ("à°¥à°¾à°‚à°•à±à°¸à±", "à° à°®à°¾à°¤à±à°°à°‚! à°®à±€à°•à± à°¸à°¹à°¾à°¯à°‚ à°šà±‡à°¯à°¡à°‚ à°¸à°‚à°¤à±‹à°·à°‚à°—à°¾ à°‰à°‚à°¦à°¿."),
    
    # Goodbye
    ("à°µà±†à°³à±à°³à±Šà°¸à±à°¤à°¾à°¨à±", "à°¸à°°à±‡, à°®à°³à±à°³à±€ à°•à°²à±à°¦à±à°¦à°¾à°‚! à°œà°¾à°—à±à°°à°¤à±à°¤!"),
    ("à°¬à±ˆ", "à°¬à±ˆ! à°®à°‚à°šà°¿ à°°à±‹à°œà± à°—à°¡à°ªà°‚à°¡à°¿!"),
    
    # General questions
    ("à°¤à±†à°²à±à°—à± à°­à°¾à°· à°—à±à°°à°¿à°‚à°šà°¿ à°šà±†à°ªà±à°ªà±", "à°¤à±†à°²à±à°—à± à°’à°• à°…à°‚à°¦à°®à±ˆà°¨ à°¦à±à°°à°¾à°µà°¿à°¡ à°­à°¾à°·. à°†à°‚à°§à±à°° à°ªà±à°°à°¦à±‡à°¶à± à°®à°°à°¿à°¯à± à°¤à±†à°²à°‚à°—à°¾à°£à°²à±‹ à°®à°¾à°Ÿà±à°²à°¾à°¡à°¤à°¾à°°à±."),
    ("à°¹à±ˆà°¦à°°à°¾à°¬à°¾à°¦à± à°—à±à°°à°¿à°‚à°šà°¿ à°šà±†à°ªà±à°ªà±", "à°¹à±ˆà°¦à°°à°¾à°¬à°¾à°¦à± à°¤à±†à°²à°‚à°—à°¾à°£ à°°à°¾à°œà°§à°¾à°¨à°¿. à°šà°¾à°°à±à°®à°¿à°¨à°¾à°°à± à°®à°°à°¿à°¯à± à°¬à°¿à°°à±à°¯à°¾à°¨à±€à°•à°¿ à°ªà±à°°à°¸à°¿à°¦à±à°§à°¿."),
    ("à°­à°¾à°°à°¤à°¦à±‡à°¶à°‚ à°—à±à°°à°¿à°‚à°šà°¿ à°šà±†à°ªà±à°ªà±", "à°­à°¾à°°à°¤à°¦à±‡à°¶à°‚ à°’à°• à°—à±Šà°ªà±à°ª à°¦à±‡à°¶à°‚. à°µà°¿à°µà°¿à°§ à°­à°¾à°·à°²à±, à°¸à°‚à°¸à±à°•à±ƒà°¤à±à°²à± à°‰à°¨à±à°¨à°¾à°¯à°¿."),
    
    # More conversational
    ("à°à°‚ à°šà±‡à°¸à±à°¤à±à°¨à±à°¨à°¾à°µà±?", "à°®à±€à°¤à±‹ à°®à°¾à°Ÿà±à°²à°¾à°¡à°Ÿà°¾à°¨à°¿à°•à°¿ à°¸à°¿à°¦à±à°§à°‚à°—à°¾ à°‰à°¨à±à°¨à°¾à°¨à±!"),
    ("à°¬à±‹à°°à± à°•à±Šà°¡à±à°¤à±‹à°‚à°¦à°¿", "à°…à°¯à±à°¯à±‹! à°à°¦à±ˆà°¨à°¾ à°•à°¥ à°šà±†à°ªà±à°ªà°®à°‚à°Ÿà°¾à°°à°¾?"),
    ("à°’à°• à°œà±‹à°•à± à°šà±†à°ªà±à°ªà±", "à°’à°• à°®à°¨à°¿à°·à°¿ à°¡à°¾à°•à±à°Ÿà°°à± à°¦à°—à±à°—à°°à°¿à°•à°¿ à°µà±†à°³à±à°³à°¾à°¡à±. à°¡à°¾à°•à±à°Ÿà°°à± à°…à°¨à±à°¨à°¾à°¡à±: à°®à±€à°°à± à°¬à°¾à°—à°¾à°¨à±‡ à°‰à°¨à±à°¨à°¾à°°à±!"),
    
    # Numbers and counting
    ("à°’à°•à°Ÿà°¿ à°¨à±à°‚à°¡à°¿ à°ªà°¦à°¿ à°µà°°à°•à± à°šà±†à°ªà±à°ªà±", "à°’à°•à°Ÿà°¿, à°°à±†à°‚à°¡à±, à°®à±‚à°¡à±, à°¨à°¾à°²à±à°—à±, à°à°¦à±, à°†à°°à±, à°à°¡à±, à°à°¨à°¿à°®à°¿à°¦à°¿, à°¤à±Šà°®à±à°®à°¿à°¦à°¿, à°ªà°¦à°¿."),
    ("à°’à°•à°Ÿà°¿ à°•à°²à°ªà°‚à°¡à°¿ à°’à°•à°Ÿà°¿", "à°’à°•à°Ÿà°¿ à°•à°²à°ªà°‚à°¡à°¿ à°’à°•à°Ÿà°¿ à°¸à°®à°¾à°¨à°‚ à°°à±†à°‚à°¡à±."),
    
    # Food
    ("à°®à±€à°•à± à°‡à°·à±à°Ÿà°®à±ˆà°¨ à°†à°¹à°¾à°°à°‚ à°à°®à°¿à°Ÿà°¿?", "à°¨à±‡à°¨à± AI à°¨à°¿, à°•à°¾à°¨à±€ à°¤à±†à°²à±à°—à± à°µà°‚à°Ÿà°•à°¾à°²à± à°šà°¾à°²à°¾ à°°à±à°šà°¿à°—à°¾ à°‰à°‚à°Ÿà°¾à°¯à°¿!"),
    ("à°¬à°¿à°°à±à°¯à°¾à°¨à±€ à°à°²à°¾ à°šà±‡à°¯à°¾à°²à°¿?", "à°¬à°¿à°°à±à°¯à°¾à°¨à±€ à°šà±‡à°¯à°¡à°¾à°¨à°¿à°•à°¿ à°¬à°¿à°¯à±à°¯à°‚, à°®à°¸à°¾à°²à°¾à°²à±, à°®à°¾à°‚à°¸à°‚ à°…à°µà°¸à°°à°‚."),
]

# Extended pairs can be generated by LLM
TOPICS_FOR_GENERATION = [
    "à°šà°°à°¿à°¤à±à°°", "à°µà°¿à°œà±à°à°¾à°¨à°‚", "à°¸à°¾à°‚à°•à±‡à°¤à°¿à°•à°¤", "à°•à±à°°à±€à°¡à°²à±", "à°¸à°‚à°—à±€à°¤à°‚",
    "à°¸à°¿à°¨à°¿à°®à°¾à°²à±", "à°†à°°à±‹à°—à±à°¯à°‚", "à°µà°¿à°¦à±à°¯", "à°ªà±à°°à°¯à°¾à°£à°‚", "à°†à°¹à°¾à°°à°‚",
    "à°•à±à°Ÿà±à°‚à°¬à°‚", "à°ªà°‚à°¡à±à°—à°²à±", "à°°à°¾à°œà°•à±€à°¯à°¾à°²à±", "à°µà±à°¯à°¾à°ªà°¾à°°à°‚", "à°ªà°°à±à°¯à°¾à°µà°°à°£à°‚"
]


class TeluguConversationGenerator:
    """Generate Telugu conversation pairs for S2S training"""
    
    def __init__(self, codec_path: str, output_dir: str, device: str = "cuda"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = device
        
        # Load YOUR codec
        print("ğŸ“¥ Loading Telugu Codec...")
        from telugu_codec_fixed import TeluCodec
        self.codec = TeluCodec().to(device)
        checkpoint = torch.load(codec_path, map_location=device)
        if 'codec_state_dict' in checkpoint:
            self.codec.load_state_dict(checkpoint['codec_state_dict'])
        else:
            self.codec.load_state_dict(checkpoint)
        self.codec.eval()
        print("âœ… Codec loaded!")
        
        # TTS will be initialized when needed
        self.tts = None
        
    async def text_to_audio(self, text: str) -> np.ndarray:
        """Convert Telugu text to audio using Edge TTS"""
        import edge_tts
        from pydub import AudioSegment
        import io
        
        communicate = edge_tts.Communicate(text, "te-IN-ShrutiNeural")
        audio_data = b""
        
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_data += chunk["data"]
        
        if not audio_data:
            return None
            
        audio_segment = AudioSegment.from_mp3(io.BytesIO(audio_data))
        audio_segment = audio_segment.set_frame_rate(16000).set_channels(1)
        
        samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
        return samples / 32768.0
    
    @torch.no_grad()
    def encode_audio(self, audio: np.ndarray) -> torch.Tensor:
        """Encode audio using YOUR codec"""
        audio_tensor = torch.from_numpy(audio).float().to(self.device)
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
        codes = self.codec.encode(audio_tensor)
        return codes.cpu()
    
    async def generate_pair(self, question: str, answer: str, pair_id: int) -> bool:
        """Generate a single conversation pair"""
        pair_dir = self.output_dir / f"pair_{pair_id:05d}"
        pair_dir.mkdir(exist_ok=True)
        
        try:
            # Generate question audio
            q_audio = await self.text_to_audio(question)
            if q_audio is None or len(q_audio) < 1600:  # Min 0.1s
                return False
            
            # Generate answer audio
            a_audio = await self.text_to_audio(answer)
            if a_audio is None or len(a_audio) < 1600:
                return False
            
            # Save audio files
            import soundfile as sf
            sf.write(pair_dir / "question.wav", q_audio, 16000)
            sf.write(pair_dir / "answer.wav", a_audio, 16000)
            
            # Encode with YOUR codec
            q_codes = self.encode_audio(q_audio)
            a_codes = self.encode_audio(a_audio)
            
            # Save codes
            torch.save(q_codes, pair_dir / "question_codes.pt")
            torch.save(a_codes, pair_dir / "answer_codes.pt")
            
            # Save metadata
            metadata = {
                "question_text": question,
                "answer_text": answer,
                "question_audio_length": len(q_audio),
                "answer_audio_length": len(a_audio),
                "question_codes_shape": list(q_codes.shape),
                "answer_codes_shape": list(a_codes.shape)
            }
            with open(pair_dir / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            print(f"Error generating pair {pair_id}: {e}")
            return False
    
    async def generate_from_templates(self, num_pairs: int):
        """Generate pairs from predefined templates"""
        print(f"\nğŸ“ Generating {num_pairs} pairs from templates...")
        
        # Expand templates with variations
        expanded_pairs = []
        for q, a in TELUGU_QA_PAIRS:
            expanded_pairs.append((q, a))
            # Add variations
            if "?" in q:
                expanded_pairs.append((q.replace("?", ""), a))
        
        # Repeat to get desired count
        while len(expanded_pairs) < num_pairs:
            expanded_pairs.extend(TELUGU_QA_PAIRS)
        
        expanded_pairs = expanded_pairs[:num_pairs]
        random.shuffle(expanded_pairs)
        
        success_count = 0
        for i, (q, a) in enumerate(tqdm(expanded_pairs, desc="Generating")):
            if await self.generate_pair(q, a, i):
                success_count += 1
            await asyncio.sleep(0.5)  # Rate limit TTS
        
        print(f"\nâœ… Generated {success_count}/{num_pairs} pairs successfully!")
        return success_count
    
    async def generate_with_llm(self, num_pairs: int, llm_model: str = "Qwen/Qwen2.5-1.5B-Instruct"):
        """Generate more diverse pairs using LLM"""
        print(f"\nğŸ¤– Generating {num_pairs} pairs using LLM...")
        
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained(llm_model)
        model = AutoModelForCausalLM.from_pretrained(
            llm_model,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        system_prompt = """Generate a Telugu question and answer pair. 
The question should be a natural conversational question in Telugu.
The answer should be a helpful, friendly response in Telugu.
Keep both short (1-2 sentences).

Format:
Q: [Telugu question]
A: [Telugu answer]"""
        
        success_count = 0
        start_id = len(list(self.output_dir.glob("pair_*")))
        
        for i in tqdm(range(num_pairs), desc="LLM Generating"):
            topic = random.choice(TOPICS_FOR_GENERATION)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Generate a Telugu Q&A about: {topic}"}
            ]
            
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.8, do_sample=True)
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # Parse Q and A
            try:
                if "Q:" in response and "A:" in response:
                    parts = response.split("A:")
                    q = parts[0].replace("Q:", "").strip()
                    a = parts[1].strip().split("\n")[0]
                    
                    if len(q) > 5 and len(a) > 5:
                        if await self.generate_pair(q, a, start_id + i):
                            success_count += 1
            except:
                pass
            
            await asyncio.sleep(0.5)
        
        print(f"\nâœ… LLM generated {success_count}/{num_pairs} pairs!")
        return success_count


async def main():
    parser = argparse.ArgumentParser(description="Generate Telugu conversation training data")
    parser.add_argument("--codec", default="best_codec.pt", help="Path to your codec")
    parser.add_argument("--output", default="data/telugu_conversations", help="Output directory")
    parser.add_argument("--num_template", type=int, default=100, help="Number of template pairs")
    parser.add_argument("--num_llm", type=int, default=0, help="Number of LLM-generated pairs")
    parser.add_argument("--device", default="cuda")
    args = parser.parse_args()
    
    generator = TeluguConversationGenerator(args.codec, args.output, args.device)
    
    total = 0
    
    if args.num_template > 0:
        total += await generator.generate_from_templates(args.num_template)
    
    if args.num_llm > 0:
        total += await generator.generate_with_llm(args.num_llm)
    
    print(f"\nğŸ‰ Total pairs generated: {total}")
    print(f"ğŸ“ Output directory: {args.output}")


if __name__ == "__main__":
    asyncio.run(main())
